% yamlstata.tex — Stata Journal insert
% To be included from main.tex provided by sjlatex

\inserttype[st0001]{article}

\author{J. P. Azevedo}{%
Jo\~ao Pedro Azevedo\\
UNICEF, Division of Data, Analytics, Planning and Monitoring\\
New York, USA\\
\texttt{jpazevedo@unicef.org}
}

\title[Reading and writing YAML in Stata]{Reading and writing YAML files in Stata:\\
A lightweight framework for reproducible and cross-platform analytics}

% \sjSetDOI{!!}  % Filled in by Stata Press

\maketitle

\begin{abstract}
YAML (``YAML Ain't Markup Language'') is a widely used, human-readable data-serialization standard that underpins configuration management in modern analytical ecosystems, including GitHub Actions, R and Python pipelines, Jupyter workflows, and cloud-based orchestration tools. Although Stata increasingly participates in multi-language, reproducible workflows, it lacks native support for reading or writing YAML. This article introduces \texttt{yaml}, a unified Stata command that provides nine subcommands for parsing, generating, querying, and validating YAML files. The command enables users to import structured configuration files into datasets or frames, work with metadata programmatically, and export updated configurations back to YAML. The implementation is lightweight, dependency-free, and targets the JSON-compatible subset of the YAML~1.2 specification. We illustrate applications in indicator metadata management using the UNICEF SDMX API, YAML-driven survey microdata harmonization, and configuration validation in automated pipelines. We conclude with recommendations for integrating YAML-based workflows into Stata-centered data production systems.
\keywords{\inserttag, yaml, reproducible research, metadata, configuration management, cross-platform workflows}
\end{abstract}


\section{Introduction}

YAML (``YAML Ain’t Markup Language'') is a widely used, human-readable data-serialization format designed for configuration files, structured metadata, and declarative workflows \citep{YAML12}. It is now ubiquitous across analytical and data-engineering ecosystems, including documentation and reporting frameworks (R Markdown, Quarto), automation platforms (GitHub Actions and other CI pipelines), and cloud-orchestration tools. To our knowledge, Stata currently lacks any general-purpose command for parsing or generating YAML files.

As empirical workflows increasingly span multiple programming environments, the absence of YAML support restricts Stata’s ability to participate in modern reproducible pipelines. Without a mechanism for reading or writing YAML, Stata scripts typically hard-code configuration values directly into do-files, leading to duplicated logic, inconsistent parameter settings, and added maintenance burden. Although Stata can interface with external languages such as Python, relying on external interpreters introduces dependency management challenges and reduces portability, especially in secure or server-based environments where Python is unavailable. A native Stata solution is therefore essential for fully reproducible workflows.

This article introduces \texttt{yaml}, a unified Stata command that enables users to read, write, query, and validate YAML configuration files entirely within Stata. The command supports a pragmatic, JSON-compatible subset of YAML 1.2 suitable for most analytical workflows encountered in practice. By allowing Stata to consume and emit the same configuration artifacts used by R, Python, and automation systems, \texttt{yaml} positions Stata as a first-class participant in cross-platform reproducible analytics.

The remainder of the article is organized as follows. Section~\ref{sec:motivation} motivates the need for YAML interoperability in Stata workflows. Section~\ref{sec:architecture} describes the command architecture and data model. Section~\ref{sec:subcommands} presents each subcommand in detail. Section~\ref{sec:applications} illustrates applications in data-production pipelines. Section~\ref{sec:discussion} discusses limitations and extensions, and Section~\ref{sec:conclusions} concludes.


\section{Motivation}
\label{sec:motivation}

\subsection{YAML in modern analytical workflows}

YAML has become a central component of contemporary data-science and engineering 
ecosystems because it provides a human-readable, structured, and platform-neutral 
way to encode configuration files, metadata, and declarative workflows \citep{YAML12}. 
Its support for nested key--value structures, lists, and simple scalar types makes 
it well suited for transparent and reproducible parameter specification. The 
emphasis on clean, well-structured data \citep{wickham14} and reproducible dynamic 
documents \citep{xie14} has further increased demand for formats that are both 
expressive and easy to maintain.

Today YAML underpins a wide range of tools and workflows, including:
\begin{itemize}
    \item GitHub Actions and other continuous-integration systems;
    \item R Markdown and Quarto project and site configuration;
    \item Python experiment configuration and workflow orchestration;
    \item agent and pipeline definitions for large-language-model systems; and
    \item cloud-infrastructure and container-orchestration templates.
\end{itemize}

As these ecosystems mature, configuration files—not code files—often become the 
primary carrier of workflow logic. YAML’s readability and interoperability make 
it a natural medium for this role.

YAML is also widely used in modern artificial-intelligence and 
large-language-model (LLM) ecosystems, where it often serves as the 
declarative layer for specifying tool interfaces, agent behaviors, and 
pipeline orchestration. Its readability and structured format make YAML 
well suited for documenting complex workflows in a way that remains 
transparent and version-controlled. While these applications lie outside 
the scope of this article, they further illustrate the central role of 
YAML as a configuration standard in emerging analytical environments.



\subsection{Stata in multi-language reproducible pipelines}

Many contemporary research teams operate mixed-language pipelines, invoking Stata for microdata processing and estimation, R for visualization and reporting, Python for orchestration and simulation, and Git-based platforms for automation and version control \citep[see][]{wdaus}. Best practices for managing such pipelines emphasize automation, explicit dependency management, and version-controlled configuration \citep{gentzkow14}. YAML files frequently serve as the stable ``contract’’ linking these components, ensuring that parameters, metadata, and execution settings remain consistent across tools.

Without YAML input and output, Stata becomes an exception in otherwise unified workflows. Analysts must duplicate configuration information inside do-files or manually translate YAML settings produced by other tools. This increases maintenance costs, introduces opportunities for inconsistencies, and reduces transparency regarding how results are generated. These limitations run counter to the growing emphasis on reproducibility across the statistical community—including multilateral organizations. The \emph{United Nations Fundamental Principles of Official Statistics} \citep{UNFPOS} explicitly highlight the importance of transparency in methods, professional standards, and documentation of statistical production processes. Reproducible analytical workflows that clearly separate configuration, metadata, and code contribute directly to these principles by enabling verifiability, auditability, and coherent statistical practice across institutions.

Although Stata can interface with external languages such as Python, relying on external interpreters reduces portability and reproducibility, particularly in secure or server-based environments where Python or its YAML libraries may not be available. A native Stata solution is therefore valuable for ensuring that Stata participates fully and consistently in cross-platform analytical systems.

\subsection{YAML for metadata management and validation}

YAML’s structured yet readable syntax makes it particularly suitable for storing statistical metadata and workflow settings, including:
\begin{itemize}
    \item indicator metadata (names, labels, units, definitions);
    \item country lists and regional classifications;
    \item thresholds, quality checks, and validation rules;
    \item API endpoints, credentials, and dataflow identifiers; and
    \item pipeline toggles or parameters controlling execution steps.
\end{itemize}

In statistical production systems—such as Sustainable Development Goal (SDG) monitoring, survey microdata harmonization, or administrative data processing—these metadata structures are often shared across multiple tools and languages. Providing Stata with direct access to YAML-based specifications reduces duplication of logic, centralizes control of configuration, and strengthens reproducibility and transparency in data-production systems. YAML files can thus serve as single sources of truth for pipeline configuration, improving consistency, auditability, and alignment with the UN Principles of Official Statistics throughout the entire analytical workflow.


\section{Architecture and data model}
\label{sec:architecture}

\subsection{Command structure}

The \texttt{yaml} command follows a subcommand architecture common in modern command-line tools and consistent with Stata programming conventions \citep{baum09}:
\begin{verbatim}
. yaml <subcommand> using filename [, options]
\end{verbatim}
This structure provides a unified entry point while allowing each operation to maintain its own syntax and option set. The command requires Stata~14.0 for core functionality; the \texttt{frame()} option requires Stata~16.0 or later.

\subsection{Storage structure}

YAML data is represented in a flat Stata dataset with explicit hierarchical references. By default, data are loaded into the current dataset. Supplying the \texttt{frame()} option loads the data into a named Stata frame, allowing multiple YAML files to coexist in memory.

The parser generates five variables:

\begin{center}
\begin{tabular}{lll}
\textbf{Variable} & \textbf{Type} & \textbf{Description} \\
\hline
\texttt{key}    & str244  & Full hierarchical key name \\
\texttt{value}  & str2000 & Value associated with the key \\
\texttt{level}  & int     & Nesting depth (1 = root) \\
\texttt{parent} & str244  & Parent key for hierarchical lookups \\
\texttt{type}   & str32   & Value type (string, numeric, boolean, parent, list\_item, null)
\end{tabular}
\end{center}

The \texttt{str2000} type for \texttt{value} was chosen to balance capacity with portability and performance. This width accommodates the vast majority of configuration values encountered in practice while maintaining compatibility with Stata~14 and avoiding the overhead of \texttt{strL} storage. Users with exceptionally long values (for example, embedded documentation blocks or large text blobs) may need to preprocess such content or store it externally.

This flat representation follows standard Stata data-processing conventions and enables key operations—such as filtering, merging, or looping—using familiar syntax without introducing nested Mata objects.


\subsection{Key naming convention}

Keys are flattened by concatenating hierarchical components with underscores. For example:

\begin{stlog}
indicators:
  CME_MRY0T4:
    label: Under-five mortality rate
    unit: Deaths per 1000 live births
\end{stlog}

is stored as:

\begin{stlog}
key                              value                        parent                 type
---------------------------------------------------------------------------------------
indicators                       (empty)                      (empty)                parent
indicators_CME_MRY0T4            (empty)                      indicators             parent
indicators_CME_MRY0T4_label      Under-five mortality rate    indicators_CME_MRY0T4  string
indicators_CME_MRY0T4_unit       Deaths per 1000 live births  indicators_CME_MRY0T4  string
\end{stlog}

Key collisions are rare in practice, but the parser retains full parent references, ensuring unambiguous identification of each YAML element.

\subsection{List item storage}

YAML lists are stored as incrementally indexed rows under a common parent:

\begin{stlog}
countries:
  - BRA
  - ARG
  - CHL
\end{stlog}

becomes:

\begin{stlog}
key            value   parent      type
-----------------------------------------------
countries      (empty) (empty)     parent
countries_1    BRA     countries   list_item
countries_2    ARG     countries   list_item
countries_3    CHL     countries   list_item
\end{stlog}

\subsection{Supported YAML subset}

The parser targets the JSON Schema subset of YAML~1.2 \citep{YAML12}, supporting:

\begin{itemize}
    \item UTF-8 encoded text,
    \item \texttt{key: value} pairs on a single line,
    \item indentation-based nesting,
    \item list items using the \texttt{- value} syntax,
    \item scalar values (strings, numbers, booleans),
    \item optional empty values (\texttt{key:}),
    \item hash-based comments (text following \texttt{\#}).
\end{itemize}

Advanced YAML features—anchors (\texttt{\&ref}), aliases (\texttt{*ref}), explicit type tags, block scalars (\texttt{|}, \texttt{>}), and flow-style mappings (\texttt{\{...\}})—are not supported. This restriction is deliberate and reflects a trade-off between completeness and robustness: most empirical configuration files rely only on the JSON-compatible subset. Malformed indentation or unsupported constructs trigger structured parser errors, ensuring transparent and reproducible behavior.

Figure~\ref{fig:architecture} summarizes the layered architecture of the \texttt{yaml} command. At the top layer, application-level use cases—such as SDMX API integration, survey harmonization, LLM-based workflows, and CI/CD pipelines—rely on configuration stored in YAML files. The user interface layer provides nine subcommands (\texttt{read}, \texttt{write}, \texttt{get}, \texttt{list}, \texttt{validate}, \texttt{describe}, \texttt{dir}, \texttt{frames}, \texttt{clear}) that mediate access to the underlying storage layer. Data are held either in the current dataset or in named frames (prefixed \texttt{yaml\_}), with each YAML entry represented by five variables (key, value, level, parent, type). The bottom layer handles file-system I/O, reading and writing YAML~1.2-compliant files.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/yaml_layers_bw.pdf}
\caption{Layered architecture of the \texttt{yaml} command. Applications at the top consume YAML-based configuration through a unified subcommand interface. The storage layer uses Stata datasets or frames with a standardized five-variable schema. The file-system layer handles I/O for YAML~1.2-compliant files.}
\label{fig:architecture}
\end{figure}


\section{Subcommands}
\label{sec:subcommands}

\subsection{yaml read}

The \texttt{yaml read} subcommand reads a YAML file and parses its contents into the current dataset (default) or into a specified frame. It is the primary entry point for importing YAML-based configuration or metadata into Stata.

\textbf{Syntax}
\begin{verbatim}
. yaml read using filename [, frame(name) locals scalars 
      prefix(string) replace verbose]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item \texttt{frame(name)} loads the output into frame \texttt{yaml\_name} instead of the current dataset (Stata~16+).
    \item \texttt{locals} also stores values as local macros in \texttt{r()}, using the naming convention \texttt{prefix}{it:key}.
    \item \texttt{scalars} also stores numeric values as Stata scalars.
    \item \texttt{prefix(string)} sets a prefix for macro or scalar names; default is \texttt{yaml\_}.
    \item \texttt{replace} replaces existing data in memory.
    \item \texttt{verbose} displays parsing progress and diagnostic output.
\end{itemize}

\textbf{Example}
\begin{stlog}
. yaml read using "config.yaml", replace
(15 keys read from config.yaml)

. yaml read using "indicators.yaml", frame(ind)
(230 keys read from indicators.yaml into frame yaml_ind)
\end{stlog}

\textbf{Stored results}
\begin{itemize}
    \item \texttt{r(n\_keys)} — number of keys parsed
    \item \texttt{r(max\_level)} — maximum nesting depth
    \item \texttt{r(filename)} — name of file read
    \item \texttt{r(frame)} — name of frame created (if \texttt{frame()} is used)
\end{itemize}

Missing or unreadable files produce error messages and no data are imported.

\subsection{yaml write}

The \texttt{yaml write} subcommand exports a Stata dataset or frame to a YAML-formatted file. It recreates hierarchical structure based on the \texttt{key}, \texttt{parent}, and \texttt{level} variables.

\textbf{Syntax}
\begin{verbatim}
. yaml write using filename [, frame(name) scalars(namelist) 
      replace verbose indent(#) header(string)]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item \texttt{frame(name)} writes YAML constructed from frame \texttt{yaml\_name} (Stata~16+).
    \item \texttt{scalars(namelist)} includes named scalars as top-level YAML pairs.
    \item \texttt{replace} overwrites an existing file of the same name.
    \item \texttt{indent(\#)} sets the number of spaces per indent level (default: 2).
    \item \texttt{header(string)} adds a comment header to the top of the file.
\end{itemize}

\textbf{Example}
\begin{stlog}
. yaml write using "output.yaml", replace
(15 keys written to output.yaml)
\end{stlog}

If the dataset does not contain valid YAML variables (\texttt{key}, \texttt{value}, \texttt{parent}, \texttt{type}), \texttt{yaml write} returns an error.

\subsection{yaml describe}

The \texttt{yaml describe} subcommand displays a compact tree-like representation of YAML data stored in the current dataset or a frame.

\textbf{Syntax}
\begin{verbatim}
. yaml describe [, frame(name) level(#)]
\end{verbatim}

\textbf{Example}
\begin{stlog}
. yaml describe
YAML structure (15 keys, max depth 3):
  name: My Project
  version: 1.0
  indicators/
    CME_MRY0T4/
      label: Under-five mortality rate
      unit: deaths per 1000 live births
\end{stlog}

The optional \texttt{level()} restricts output to a maximum indentation depth.

\subsection{yaml list}

The \texttt{yaml list} subcommand lists keys and/or values, optionally restricted by a parent key. It is useful for loops, metadata queries, and programmatic YAML exploration.

\textbf{Syntax}
\begin{verbatim}
. yaml list [parent] [, frame(name) keys values 
      separator(string) children stata noheader]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item \texttt{keys} returns key names in \texttt{r(keys)}.
    \item \texttt{values} returns values in \texttt{r(values)}.
    \item \texttt{children} returns only immediate children of the specified parent.
    \item \texttt{separator(string)} specifies the delimiter (default: space).
    \item \texttt{stata} formats returned lists in Stata compound quotes for looping.
    \item \texttt{noheader} suppresses printed output.
\end{itemize}

If the parent is not found, the returned lists are empty and \texttt{r(found)} = 0.

\textbf{Example}
\begin{stlog}
. yaml list indicators, keys children
Keys under indicators: CME_MRY0T4 CME_MRY0

. return list
r(keys) : "CME_MRY0T4 CME_MRY0"
\end{stlog}

\subsection{yaml get}

The \texttt{yaml get} subcommand retrieves all attributes (children) associated with a specific key and returns them in separate \texttt{r()} macros.

\textbf{Syntax}
\begin{verbatim}
. yaml get parent:keyname | keyname [, frame(name) attributes(namelist) quiet]
\end{verbatim}

The colon notation (\texttt{parent:key}) restricts the search to a specific branch.

If a key is not found, \texttt{r(found)} = 0 and no attributes are returned.

\textbf{Example}
\begin{stlog}
. yaml get indicators:CME_MRY0T4
  label: Under-five mortality rate
  unit: Deaths per 1000 live births
  dataflow: CME

. return list
r(key)      : "CME_MRY0T4"
r(parent)   : "indicators"
r(found)    : "1"
r(n_attrs)  : "3"
\end{stlog}

\subsection{yaml validate}

The \texttt{yaml validate} subcommand checks that specified keys exist and that selected keys match required value types.

\textbf{Syntax}
\begin{verbatim}
. yaml validate [, frame(name) required(keylist) 
      types(key:type ...) quiet]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item \texttt{required(keylist)} verifies that all listed keys exist.
    \item \texttt{types(key:type)} verifies that each key has the expected type (numeric, string, boolean).
    \item \texttt{quiet} suppresses printed output; results are returned silently in \texttt{r()}.
\end{itemize}

If validation fails, \texttt{r(valid)} = 0 and \texttt{r(n\_errors)} reports the number of issues.

\textbf{Example}
\begin{stlog}
. yaml validate, required(name version database) ///
>     types(database_port:numeric debug:boolean)
Validation passed (3 required keys, 2 type checks)
\end{stlog}

\subsection{yaml dir}

The \texttt{yaml dir} subcommand provides an inventory of all YAML data currently in memory, including data stored in the active dataset and in frames created via \texttt{yaml read}.

\textbf{Syntax}
\begin{verbatim}
. yaml dir [, detail]
\end{verbatim}

YAML data are detected by (1) the presence of standard YAML variables and (2) the dataset characteristic \texttt{\_dta[yaml\_source]} set by \texttt{yaml read}.

\textbf{Example}
\begin{stlog}
. yaml read using "config.yaml", replace
. yaml dir, detail
\end{stlog}

\subsection{yaml frames}

The \texttt{yaml frames} subcommand lists only YAML-related frames, without reporting on the current dataset. This mirrors \texttt{frames dir} but is restricted to frames created by \texttt{yaml read}.

\textbf{Syntax}
\begin{verbatim}
. yaml frames [, detail]
\end{verbatim}

\textbf{Example}
\begin{stlog}
. yaml frames, detail
------------------------------------------------------------
YAML Frames in Memory
------------------------------------------------------------
  1. dev  (12 entries)
  2. prod (12 entries)
------------------------------------------------------------
\end{stlog}

\subsection{yaml clear}

The \texttt{yaml clear} subcommand removes YAML data from memory—either from the current dataset, from a specific frame, or from all YAML-related frames.

\textbf{Syntax}
\begin{verbatim}
. yaml clear [framename] [, all]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item no argument — clear YAML variables from the current dataset.
    \item \texttt{framename} — drop the frame \texttt{yaml\_framename}.
    \item \texttt{all} — drop all frames created by \texttt{yaml read}.
\end{itemize}

\textbf{Example}
\begin{stlog}
. yaml clear config
(yaml_config dropped)

. yaml clear, all
(3 yaml frames dropped)
\end{stlog}


\section{Applications}
\label{sec:applications}

This section illustrates how the \texttt{yaml} command enables Stata to operate within configuration-driven, multi-language analytical workflows. The examples reflect common patterns in UNICEF systems, including SDMX-based indicator retrieval, survey microdata harmonization, pipeline validation, environment management through frames, and cross-platform standardization of Git-based repositories. Across these use cases, the separation of configuration from execution improves reproducibility, auditability, and interoperability in line with the UN Principles of Official Statistics.

\subsection{Indicator metadata management with the UNICEF SDMX API}

Many UNICEF analytics pipelines rely on structured metadata for indicator names, units, SDG mappings, and API-related details. YAML provides a single, readable format for storing such metadata in a form that can be shared across Stata, R, and Python. For example:

\begin{stlog}
indicators:
  CME_MRY0T4:
    name: Under-five mortality rate
    unit: Deaths per 1,000 live births
    dataflow: CME
    sdg_target: "3.2.1"
  NT_ANT_HAZ_NE2_MOD:
    name: Stunting prevalence (moderate and severe)
    unit: Percent
    dataflow: NUTRITION
    sdg_target: "2.2.1"
  IM_DTP3:
    name: DTP3 immunization coverage
    unit: Percent
    dataflow: IMMUNISATION
    sdg_target: "3.b.1"
\end{stlog}

The metadata can be imported into Stata and stored in a frame for later reference:

\begin{stlog}
. yaml read using "unicef_indicators.yaml", frame(meta)
(9 keys read into frame yaml_meta)

. yaml list indicators, keys children frame(meta)
Keys under indicators: CME_MRY0T4 NT_ANT_HAZ_NE2_MOD IM_DTP3
\end{stlog}

Because the SDMX REST API requires a dataflow identifier, YAML metadata can be used to construct URL queries programmatically:

\begin{stlog}
. local api_base "https://sdmx.data.unicef.org/ws/public/sdmxapi/rest/data"

. yaml get indicators:CME_MRY0T4, frame(meta) quiet
. local flow "`r(dataflow)'"
. local api_url "`api_base'/UNICEF,`flow',1.0/.CME_MRY0T4.?format=csv"

. import delimited "`api_url'", clear
(18 vars, 5,847 obs)
\end{stlog}

Labels can be drawn from the same YAML metadata:

\begin{stlog}
. yaml get indicators:CME_MRY0T4, frame(meta) quiet
. label variable obs_value "`r(name)' (`r(unit)')"
\end{stlog}

A full indicator batch can be looped over easily:

\begin{stlog}
. yaml list indicators, keys children frame(meta)
. local indicators "`r(keys)'"

. foreach ind of local indicators {
      yaml get indicators:`ind', frame(meta) quiet
      local flow "`r(dataflow)'"
      local url "`api_base'/UNICEF,`flow',1.0/.`ind'.?format=csv"

      import delimited "`url'", clear
      label variable obs_value "`r(name)' (`r(unit)')"
      label data "`r(name)' - SDG `r(sdg_target)'"
      save "`ind'.dta", replace
  }
\end{stlog}

This pattern ensures that indicator definitions remain consistent across the entire workflow and across analytical languages.

\subsection{YAML-driven microdata harmonization}

Harmonizing survey microdata often requires mapping a variety of source variables to harmonized names, applying consistent labels, and recoding categorical values. Encoding this logic in YAML keeps harmonization decisions separate from the Stata code that executes them.

\begin{stlog}
survey: MICS6
country: ETH
year: 2024

variables:
  hhid:
    from: HH1
    to: hhid
    label: Household ID

  male:
    from: HL4
    to: male
    label: Sex (1=Male, 0=Female)
    values:
      - from: 1
        to: 1
        label: Male
      - from: 2
        to: 0
        label: Female
\end{stlog}

A harmonization script can use this configuration as follows:

\begin{stlog}
. yaml read using "mics_eth_config.yaml", frame(config)
. yaml get input_file, frame(config) quiet
. use "`r(value)'", clear

. yaml list variables, keys children frame(config)
. local varlist "`r(keys)'"

. foreach var of local varlist {
      yaml get variables:`var', frame(config) quiet
      rename `r(from)' `r(to)'
      label variable `r(to)' "`r(label)'"

      yaml list variables:`var':values, keys children frame(config)
      if "`r(keys)'" != "" {
          local recode_str ""
          label define `r(to)'_lbl, replace

          foreach v of local r(keys) {
              yaml get variables:`var':values:`v', frame(config) quiet
              local recode_str "`recode_str' (`r(from)' = `r(to)')"
              if "`r(to)'" != "." label define `r(to)'_lbl `r(to)' "`r(label)'", add
          }

          recode `r(to)' `recode_str'
          label values `r(to)' `r(to)'_lbl
      }
  }
\end{stlog}

This separation of **harmonization logic** (YAML) from **execution** (Stata) improves traceability, documentation, and cross-language interoperability.

\subsection{Configuration validation}

In automated pipelines, configuration files often require strict validation before processing begins. The \texttt{yaml validate} subcommand provides a convenient mechanism:

\begin{stlog}
. yaml read using "pipeline_config.yaml", replace

. yaml validate, required(name version database api_endpoint) ///
      types(database_port:numeric api_timeout:numeric debug:boolean)

. if (r(valid) == 0) {
      di as error "Configuration validation failed!"
      exit 198
  }
\end{stlog}

Validation ensures that configuration errors are caught early, supporting transparent governance and predictable execution.

\subsection{Working with multiple YAML files}

Many production pipelines maintain separate development and production configurations. With Stata frames, multiple configurations can be loaded simultaneously:

\begin{stlog}
. yaml read using "dev_config.yaml", frame(dev)
. yaml read using "prod_config.yaml", frame(prod)
\end{stlog}

A consolidated inventory can be obtained using:

\begin{stlog}
. yaml frames, detail
------------------------------------------------------------
YAML Frames in Memory
------------------------------------------------------------
  1. dev  (12 entries)
  2. prod (12 entries)
------------------------------------------------------------
\end{stlog}

Environment-specific settings can then be inspected or compared programmatically:

\begin{stlog}
. yaml get database:host, frame(dev)
. local dev_host "`r(host)'"

. yaml get database:host, frame(prod)
. local prod_host "`r(host)'"

. di "Dev: `dev_host', Prod: `prod_host'"
Dev: localhost, Prod: db.production.example.com
\end{stlog}

This pattern supports clean separation of configuration by environment.

\subsection{Round-trip: reading and writing YAML from Stata}

Beyond consuming YAML, Stata can also modify and regenerate configuration files, enabling it to participate actively in configuration management:

\begin{stlog}
. yaml read using "config.yaml", replace

. replace value = "3.0" if key == "version"
. replace value = "`c(current_date)'" if key == "last_modified"

. yaml write using "config_updated.yaml", replace
\end{stlog}

Round-trip processing is useful for timestamping, version increments, and recording workflow status indicators in automated pipelines.

\subsection{Standardizing Git-based project profiles across tools}

UNICEF analytical projects frequently span Stata, R, and Python and are managed in Git-based repositories. Each analyst typically maintains a local clone and personal file paths. Historically, Stata profile do-files contained long username-specific blocks to resolve folder locations. These were difficult to scale and error-prone.

A YAML configuration kept outside the repository provides a more robust and interoperable solution:

\begin{stlog}
\# ~/.config/user_config.yml

analyst1:
  githubFolder: "D:/Users/analyst1/GitHub"
  teamsRoot:   "C:/Users/analyst1/TeamsRoot"

analyst2:
  githubFolder: "C:/Users/analyst2/Projects/GitHub"
  teamsRoot:   "C:/Users/analyst2/Shared/TeamsRoot"
\end{stlog}

Stata can then resolve the correct paths dynamically:

\begin{stlog}
. local user = c(username)
. yaml read using "~/.config/user_config.yml", frame(usercfg)
. yaml get `user', frame(usercfg) quiet
. cd "`r(githubFolder)'/ProjectName"
\end{stlog}

The same YAML configuration can be consumed by R:

\begin{stlog}
cfg  <- yaml::read_yaml("~/.config/user_config.yml")
user <- Sys.getenv("USERNAME")
root <- cfg[[user]]$githubFolder
\end{stlog}

and Python:

\begin{stlog}
with open("~/.config/user_config.yml") as f:
    cfg = yaml.safe_load(f)
root = cfg[os.environ["USERNAME"]]["githubFolder"]
\end{stlog}

This practice establishes a single source of truth for environment-specific paths, strengthening reproducibility, reducing setup friction for new team members, and supporting consistent behavior across Stata, R, and Python in line with the UN Principles of Official Statistics.

\subsection*{Recommended practices}

In practice, we have found a few patterns particularly useful when integrating \texttt{yaml} into Stata workflows: (i) store shared analytical configuration (indicator lists, harmonization maps, pipeline toggles) in YAML files under version control; (ii) store user- and machine-specific settings (local paths, credentials) in YAML files outside the repository, as in the Git-based profile example; and (iii) keep secrets and tokens in separate, access-controlled configuration files rather than in project-level YAML. Together, these patterns help implement a “single source of truth” for configuration while aligning with good practices for reproducibility and data governance.

\section{Discussion}
\label{sec:discussion}
The \texttt{yaml} command positions Stata as a first-class participant in modern multi-language analytical ecosystems. Although YAML has long been central to workflows in R, Python, and cloud-computing environments, Stata has lacked native support for reading or writing YAML-based configuration files. The implementation presented here follows established Stata programming conventions \citep{cox05d} and provides a lightweight bridge between Stata and the broader reproducible-research infrastructure increasingly used across data science and official statistics.

Several features of the design are particularly valuable for empirical research:

\begin{itemize}
    \item \emph{Portability}: YAML is widely supported, human-readable, and naturally version-controlled.
    \item \emph{Reproducibility}: externalizing configuration parameters reduces hidden assumptions and promotes stable, auditable workflows.
    \item \emph{Interoperability}: Stata can now consume the same configuration artifacts used by R, Python, Quarto, GitHub Actions, and cloud-orchestration tools.
    \item \emph{Transparency}: the chosen JSON-compatible subset of YAML is simple enough to inspect and edit by hand, aligning with best practices for documentation and governance.
    \item \emph{No dependencies}: the implementation uses only built-in Stata functionality, avoiding reliance on Python or external libraries.
\end{itemize}

The design also resonates with \citet{knuth84}'s principles of literate programming, which emphasize making analytical logic understandable to humans rather than merely executable by machines. YAML-based configuration naturally supports this philosophy: by externalizing parameters, metadata, and workflow settings into readable files, analysts create systems in which the ``what'' and ``why'' of an analysis are documented alongside the ``how.'' The microdata harmonization example in Section~\ref{sec:applications} illustrates this directly, showing how YAML can simultaneously serve as configuration, documentation, and audit trail. This separation of concerns reflects widely endorsed principles for transparent statistical production, including the United Nations Fundamental Principles of Official Statistics.

The current implementation has limitations. It deliberately targets only the JSON-schema subset of the YAML~1.2 specification (Chapter~10.2). Advanced features such as anchors, aliases, explicit type tags, block scalars, and flow-style mappings are not supported. Parsing relies on indentation rules, so malformed YAML results in structured, human-readable error messages rather than partial recovery. These constraints reflect a deliberate focus on simplicity, predictability, and reproducibility for configuration files, even though they may limit applicability to more complex or free-form YAML documents.

In practice, YAML configuration files used in UNICEF and similar analytical pipelines are modest in size (tens to a few hundred keys). On such files, parsing and frame creation typically complete in well under a second on standard hardware. The implementation processes files linearly and does not depend on external libraries, so performance scales approximately with the number of keys and lines in the YAML file. For very large or highly nested YAML documents, it may be more appropriate to preprocess the configuration in a dedicated YAML tool and pass a simplified subset to Stata.

An alternative approach is to rely on Stata’s integration with external languages (for example, calling PyYAML via \texttt{python}) or to constrain configuration to JSON only. These strategies can be effective in some settings but introduce external dependencies, complicate deployment in secure or server-based environments, and limit the ability to inspect or edit configuration directly from within Stata. The \texttt{yaml} command complements such approaches by providing a native interface to a widely used configuration language while maintaining maximal portability.

YAML is also increasingly central to artificial-intelligence and 
large-language-model workflows, where it is used to specify tool 
interfaces, agent configurations, and orchestration logic for multi-step 
pipelines. As these systems become more integrated into statistical 
production and analytical research, native YAML support helps ensure that 
Stata can interoperate with—and contribute to—such emerging infrastructures.


Future extensions could include:

\begin{itemize}
    \item a Mata-based associative dictionary interface layered on top of the frame representation for richer querying;
    \item helper tools for emitting YAML variants tailored to specific infrastructures (such as GitHub Actions or Quarto metadata blocks);
    \item support for block scalars (\texttt{|} and \texttt{>}) to accommodate multi-line text; and
    \item interactive tools for inspecting or editing YAML directly within Stata.
\end{itemize}

These enhancements would further strengthen Stata’s interoperability with configuration-driven pipelines and expand the scope of workflows that can be expressed natively in YAML.


\section{Conclusions}
\label{sec:conclusions}

As analytical pipelines increasingly span multiple programming environments, the ability to consume and generate human-readable configuration files has become essential for reproducible research. YAML has emerged as a standard configuration language across data science, underpinning workflows in R, Python, Quarto, DevOps systems, and cloud infrastructure. The \texttt{yaml} command introduced in this article brings this capability to Stata in a lightweight and dependency-free manner.

The unified subcommand architecture—comprising \texttt{read}, \texttt{write}, \texttt{describe}, \texttt{list}, \texttt{get}, \texttt{validate}, \texttt{dir}, \texttt{frames}, and \texttt{clear}—provides a comprehensive yet coherent interface for working with YAML files. This design allows analysts to externalize configuration parameters, metadata structures, and validation rules, thereby improving reproducibility, transparency, and auditability within statistical workflows. Stata scripts can now participate seamlessly in multi-language pipelines, consume configuration artifacts shared across tools, and even update pipeline metadata programmatically.

By enabling Stata to interoperate directly with YAML-based workflows, the \texttt{yaml} command strengthens its role in interdisciplinary analytical ecosystems and supports good statistical governance practices, including those emphasized by the United Nations Fundamental Principles of Official Statistics. The approach facilitates cleaner metadata management, more flexible and maintainable pipeline design, and more consistent results across platforms—an increasingly important requirement in collaborative empirical research.

This capability is particularly relevant for national statistical offices and international agencies that rely on mixed-language pipelines to produce official statistics. Clear configuration management and cross-tool coherence enhance the transparency, reproducibility, and professional standards required for the production of credible and trustworthy official statistics.


\section*{Acknowledgments}

The author thanks the Stata Journal editors and anonymous reviewers for their constructive feedback. The views expressed in this article are those of the author and do not necessarily reflect the official position of UNICEF.

\bibliographystyle{sj}
\bibliography{sj}


\begin{aboutauthor}
Jo\~ao Pedro Azevedo is Deputy Director and Chief Statistician in UNICEF's Division of Data, Analytics, Planning and Monitoring. He works on official statistics and the development of reproducible, cross-platform, and scalable analytical pipelines that support the production and use of child-related data in global monitoring systems, with the aim of generating policy-relevant insights and informing action on the ground.
\end{aboutauthor}

