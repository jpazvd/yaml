% jpazvd-yaml-article.tex — Stata Journal article content
% To be included from jpazvd-yaml.tex
% Version 1.9.0 with high-performance Mata parser, collapse filters, indicators preset

\inserttype[st0001]{article}

\author{J. P. Azevedo}{%
Jo\~ao Pedro Azevedo\\
Office of the Chief Statistician, UNICEF\\
New York, USA\\
\texttt{jpazevedo@unicef.org}
}

\title[Reading and writing YAML in Stata]{Reading and writing YAML files in Stata:\\
A lightweight, dependency-free implementation}

% \sjSetDOI{!!}  % Filled in by Stata Press

\maketitle

\begin{abstract}
YAML (``YAML Ain't Markup Language'') is a widely used, human-readable data-serialization standard that underpins configuration management in modern analytical ecosystems, including GitHub Actions, R and Python pipelines, and cloud-based orchestration tools. Although Stata increasingly participates in multi-language, reproducible workflows, it lacks native support for reading or writing YAML. This article introduces \texttt{yaml}, a unified Stata command with nine subcommands for parsing, generating, querying, and validating YAML files. The implementation is lightweight, dependency-free, and targets the JSON-compatible subset of the YAML~1.2 specification. Three parsing modes address different scale requirements: a canonical parser for general use, a fast-read mode with field-selective extraction and frame-based caching for large catalogs, and a Mata-based bulk parser that produces wide-format output via the \texttt{collapse} option with filter controls (\texttt{colfields}, \texttt{maxlevel}). The \texttt{indicators} preset provides one-step parsing of wbopendata and unicefdata indicator metadata. We illustrate applications in SDMX-based indicator metadata management, YAML-driven survey microdata harmonization, and configuration validation in automated pipelines.
\keywords{\inserttag, yaml, reproducible research, metadata, configuration management, cross-platform workflows}
\end{abstract}


\section{Introduction}

YAML (``YAML Ain’t Markup Language'') is a widely used, human-readable data-serialization format designed for configuration files, structured metadata, and declarative workflows \citep{YAML12}. It is now ubiquitous across analytical and data-engineering ecosystems, including documentation and reporting frameworks (R Markdown, Quarto), automation platforms (GitHub Actions and other CI pipelines), and cloud-orchestration tools. To our knowledge, Stata currently lacks any general-purpose command for parsing or generating YAML files.

As empirical workflows increasingly span multiple programming environments, the absence of YAML support restricts Stata’s ability to participate in modern reproducible pipelines. Without a mechanism for reading or writing YAML, Stata scripts typically hard-code configuration values directly into do-files, leading to duplicated logic, inconsistent parameter settings, and added maintenance burden. Although Stata can interface with external languages such as Python, relying on external interpreters introduces dependency management challenges and reduces portability, especially in secure or server-based environments where Python is unavailable. A native Stata solution is therefore essential for fully reproducible workflows.

This article introduces \texttt{yaml}, a unified Stata command that enables users to read, write, query, and validate YAML configuration files entirely within Stata. The command supports a pragmatic, JSON-compatible subset of YAML 1.2 suitable for most analytical workflows encountered in practice. By allowing Stata to consume and emit the same configuration artifacts used by R, Python, and automation systems, \texttt{yaml} positions Stata as a first-class participant in cross-platform reproducible analytics.

The remainder of the article is organized as follows. Section~\ref{sec:motivation} motivates the need for YAML interoperability in Stata workflows. Section~\ref{sec:architecture} describes the command architecture and data model. Section~\ref{sec:subcommands} presents each subcommand in detail. Section~\ref{sec:applications} illustrates applications in data-production pipelines. Section~\ref{sec:discussion} discusses limitations and extensions, and Section~\ref{sec:conclusions} concludes.


\section{Motivation}
\label{sec:motivation}

\subsection{YAML in modern analytical workflows}

YAML has become a central component of contemporary data-science and engineering 
ecosystems because it provides a human-readable, structured, and platform-neutral 
way to encode configuration files, metadata, and declarative workflows \citep{YAML12}. 
Its support for nested key--value structures, lists, and simple scalar types makes 
it well suited for transparent and reproducible parameter specification. The 
emphasis on clean, well-structured data \citep{wickham14} and reproducible dynamic 
documents \citep{xie14} has further increased demand for formats that are both 
expressive and easy to maintain.

Today YAML underpins a wide range of tools and workflows, including:
\begin{itemize}
    \item GitHub Actions and other continuous-integration systems;
    \item R Markdown and Quarto project and site configuration;
    \item Python experiment configuration and workflow orchestration;
    \item agent and pipeline definitions for large-language-model systems; and
    \item cloud-infrastructure and container-orchestration templates.
\end{itemize}

As these ecosystems mature, configuration files—not code files—often become the 
primary carrier of workflow logic. YAML’s readability and interoperability make 
it a natural medium for this role.

YAML is also widely used in modern artificial-intelligence and 
large-language-model (LLM) ecosystems, where it often serves as the 
declarative layer for specifying tool interfaces, agent behaviors, and 
pipeline orchestration. Its readability and structured format make YAML 
well suited for documenting complex workflows in a way that remains 
transparent and version-controlled. While these applications lie outside 
the scope of this article, they further illustrate the central role of 
YAML as a configuration standard in emerging analytical environments.



\subsection{Stata in multi-language reproducible pipelines}

Many contemporary research teams operate mixed-language pipelines, invoking Stata for microdata processing and estimation, R for visualization and reporting, Python for orchestration and simulation, and Git-based platforms for automation and version control \citep[see][]{wdaus}. Best practices for managing such pipelines emphasize automation, explicit dependency management, and version-controlled configuration \citep{gentzkow14}. YAML files frequently serve as the stable ``contract’’ linking these components, ensuring that parameters, metadata, and execution settings remain consistent across tools.

Without YAML input and output, Stata becomes an exception in otherwise unified workflows. Analysts must duplicate configuration information inside do-files or manually translate YAML settings produced by other tools. This increases maintenance costs, introduces opportunities for inconsistencies, and reduces transparency regarding how results are generated. These limitations run counter to the growing emphasis on reproducibility across the statistical community—including multilateral organizations. The \emph{United Nations Fundamental Principles of Official Statistics} \citep{UNFPOS} explicitly highlight the importance of transparency in methods, professional standards, and documentation of statistical production processes. Reproducible analytical workflows that clearly separate configuration, metadata, and code contribute directly to these principles by enabling verifiability, auditability, and coherent statistical practice across institutions.

Although Stata can interface with external languages such as Python, relying on external interpreters reduces portability and reproducibility, particularly in secure or server-based environments where Python or its YAML libraries may not be available. A native Stata solution is therefore valuable for ensuring that Stata participates fully and consistently in cross-platform analytical systems.

\subsection{YAML for metadata management and validation}

YAML’s structured yet readable syntax makes it particularly suitable for storing statistical metadata and workflow settings, including:
\begin{itemize}
    \item indicator metadata (names, labels, units, definitions);
    \item country lists and regional classifications;
    \item thresholds, quality checks, and validation rules;
    \item API endpoints, credentials, and dataflow identifiers; and
    \item pipeline toggles or parameters controlling execution steps.
\end{itemize}

In statistical production systems—such as Sustainable Development Goal (SDG) monitoring, survey microdata harmonization, or administrative data processing—these metadata structures are often shared across multiple tools and languages. Providing Stata with direct access to YAML-based specifications reduces duplication of logic, centralizes control of configuration, and strengthens reproducibility and transparency in data-production systems. YAML files can thus serve as single sources of truth for pipeline configuration, improving consistency, auditability, and alignment with the UN Principles of Official Statistics throughout the entire analytical workflow.


\section{Architecture and data model}
\label{sec:architecture}

\subsection{Command structure}

The \texttt{yaml} command follows a subcommand architecture common in modern command-line tools and consistent with Stata programming conventions \citep{baum09}:
\begin{verbatim}
. yaml <subcommand> using filename [, options]
\end{verbatim}
This structure provides a unified entry point while allowing each operation to maintain its own syntax and option set. The command requires Stata~14.0 for core functionality; the \texttt{frame()} option requires Stata~16.0 or later.

\subsection{Storage structure}

YAML data is represented in a flat Stata dataset with explicit hierarchical references. By default, data are loaded into the current dataset. Supplying the \texttt{frame()} option loads the data into a named Stata frame, allowing multiple YAML files to coexist in memory.

The parser generates five variables:

\begin{center}
\begin{tabular}{lll}
\textbf{Variable} & \textbf{Type} & \textbf{Description} \\
\hline
\texttt{key}    & str244  & Full hierarchical key name \\
\texttt{value}  & str2000 & Value associated with the key \\
\texttt{level}  & int     & Nesting depth (1 = root) \\
\texttt{parent} & str244  & Parent key for hierarchical lookups \\
\texttt{type}   & str32   & Value type (string, numeric, boolean, parent, list\_item, null)
\end{tabular}
\end{center}

The \texttt{str2000} type for \texttt{value} was chosen to balance capacity with portability and performance. This width accommodates the vast majority of configuration values encountered in practice while maintaining compatibility with Stata~14 and avoiding the overhead of \texttt{strL} storage. For values exceeding 2045~characters (such as embedded documentation blocks or large text blobs), the \texttt{strl} option stores values as \texttt{strL} instead, supporting arbitrarily long content.

This flat representation follows standard Stata data-processing conventions and enables key operations—such as filtering, merging, or looping—using familiar syntax without introducing nested Mata objects.


\subsection{Key naming convention}

Keys are flattened by concatenating hierarchical components with underscores. For example:

\begin{stlog}
indicators:
  CME_MRY0T4:
    label: Under-five mortality rate
    unit: Deaths per 1000 live births
\end{stlog}

is stored as:

\begin{stlog}
key                              value                        parent                 type
---------------------------------------------------------------------------------------
indicators                       (empty)                      (empty)                parent
indicators_CME_MRY0T4            (empty)                      indicators             parent
indicators_CME_MRY0T4_label      Under-five mortality rate    indicators_CME_MRY0T4  string
indicators_CME_MRY0T4_unit       Deaths per 1000 live births  indicators_CME_MRY0T4  string
\end{stlog}

Key collisions are rare in practice, but the parser retains full parent references, ensuring unambiguous identification of each YAML element.

\subsection{List item storage}

YAML lists are stored as incrementally indexed rows under a common parent:

\begin{stlog}
countries:
  - BRA
  - ARG
  - CHL
\end{stlog}

becomes:

\begin{stlog}
key            value   parent      type
-----------------------------------------------
countries      (empty) (empty)     parent
countries_1    BRA     countries   list_item
countries_2    ARG     countries   list_item
countries_3    CHL     countries   list_item
\end{stlog}

\subsection{Supported YAML subset}

The parser targets the JSON Schema subset of YAML~1.2 \citep{YAML12}, supporting:

\begin{itemize}
    \item UTF-8 encoded text,
    \item \texttt{key: value} pairs on a single line,
    \item indentation-based nesting,
    \item list items using the \texttt{- value} syntax,
    \item scalar values (strings, numbers, booleans),
    \item optional empty values (\texttt{key:}),
    \item hash-based comments (text following \texttt{\#}).
\end{itemize}

Advanced YAML features\textemdash anchors (\texttt{\&ref}), aliases (\texttt{*ref}), explicit type tags, and flow-style mappings (\texttt{\{...\}})\textemdash are not supported by either parser. Block scalars (\texttt{|}, \texttt{>}) are supported in fast-read mode via the \texttt{blockscalars} option and in the canonical parser. These restrictions reflect a deliberate trade-off between completeness and robustness: most empirical configuration files rely only on the JSON-compatible subset. Malformed indentation or unsupported constructs trigger structured parser errors, ensuring transparent and reproducible behavior.

\subsection{Fast-read data model}
\label{sec:fastread}

The canonical parser produces a rich five-variable schema suitable for arbitrary YAML structures. For large, regularly structured catalogs—such as indicator metadata files with hundreds of top-level entries—a faster alternative is available via the \texttt{fastread} option. The fast-read parser trades generality for speed by assuming a shallow two-level mapping (top-level keys with flat attribute sets) and produces a different output schema:

\begin{center}
\begin{tabular}{lll}
\textbf{Variable} & \textbf{Type} & \textbf{Description} \\
\hline
\texttt{key}   & str244  & Top-level key name \\
\texttt{field} & str244  & Attribute name within the key \\
\texttt{value} & str2000 & Scalar value of the attribute \\
\texttt{list}  & str2000 & Extracted list items (if \texttt{listkeys()} specified) \\
\texttt{line}  & int     & Source file line number
\end{tabular}
\end{center}

Each row represents one attribute of one top-level key. This row-per-field layout enables direct use of \texttt{levelsof}, \texttt{merge}, and other standard Stata operations without regex-based key parsing. The \texttt{fields()} option further reduces output to only the named attributes, and \texttt{cache()} stores parsed results in a named frame for instant retrieval on subsequent calls to the same file.

Figure~\ref{fig:architecture} summarizes the layered architecture of the \texttt{yaml} command. At the top layer, application-level use cases—such as SDMX API integration, survey harmonization, LLM-based workflows, and CI/CD pipelines—rely on configuration stored in YAML files. The user interface layer provides nine subcommands (\texttt{read}, \texttt{write}, \texttt{get}, \texttt{list}, \texttt{validate}, \texttt{describe}, \texttt{dir}, \texttt{frames}, \texttt{clear}) that mediate access to the underlying storage layer. Data are held either in the current dataset or in named frames (prefixed \texttt{yaml\_}), with each YAML entry represented by five variables (key, value, level, parent, type). The bottom layer handles file-system I/O, reading and writing YAML~1.2-compliant files.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{figures/yaml_layers_bw.pdf}
\caption{Layered architecture of the \texttt{yaml} command. Applications at the top consume YAML-based configuration through a unified subcommand interface. The storage layer uses Stata datasets or frames with a standardized five-variable schema. The file-system layer handles I/O for YAML~1.2-compliant files.}
\label{fig:architecture}
\end{figure}


\section{Subcommands}
\label{sec:subcommands}

\subsection{yaml read}

The \texttt{yaml read} subcommand reads a YAML file and parses its contents into the current dataset (default) or into a specified frame. It is the primary entry point for importing YAML-based configuration or metadata into Stata.

\textbf{Syntax}
\begin{verbatim}
. yaml read using filename [, frame(name) locals scalars
      prefix(string) replace verbose fastread fields(string)
      listkeys(string) cache(string) targets(string) earlyexit
      stream index(string) blockscalars bulk collapse
      colfields(string) maxlevel(#) indicators strl]
\end{verbatim}

\textbf{Core options}
\begin{itemize}
    \item \texttt{frame(name)} loads the output into frame \texttt{yaml\_name} instead of the current dataset (Stata~16+).
    \item \texttt{locals} also stores values as local macros in \texttt{r()}, using the naming convention \texttt{prefix}{it:key}.
    \item \texttt{scalars} also stores numeric values as Stata scalars.
    \item \texttt{prefix(string)} sets a prefix for macro or scalar names; default is \texttt{yaml\_}.
    \item \texttt{replace} replaces existing data in memory.
    \item \texttt{verbose} displays parsing progress and diagnostic output.
\end{itemize}

\textbf{Fast-read options} (opt-in speed-first parser)
\begin{itemize}
    \item \texttt{fastread} activates the fast-read parser, producing a flat table with columns \texttt{key}, \texttt{field}, \texttt{value}, \texttt{list}, \texttt{line} (see Section~\ref{sec:fastread}). Not compatible with \texttt{locals} or \texttt{scalars}.
    \item \texttt{fields(string)} restricts fast-read output to the named fields, reducing memory use and parse time.
    \item \texttt{listkeys(string)} extracts inline YAML lists for the specified fields into the \texttt{list} column.
    \item \texttt{cache(string)} stores the parsed result in a named frame cache; subsequent calls with the same file and cache name return the cached data instantly (Stata~16+). \texttt{r(cache\_hit)} indicates whether the cache was used.
    \item \texttt{blockscalars} enables capture of multi-line block scalars (\texttt{|} and \texttt{>}) in fast-read mode.
\end{itemize}

\textbf{Canonical-parser options}
\begin{itemize}
    \item \texttt{targets(string)} limits parsing to the specified key names; only targeted keys (and their ancestors) are stored, substantially reducing parse time for large files.
    \item \texttt{earlyexit} stops parsing as soon as all \texttt{targets()} have been found. Requires \texttt{targets()}.
    \item \texttt{stream} uses streaming tokenization for lower memory overhead on very large files.
    \item \texttt{index(string)} materializes an auxiliary index frame for faster repeated lookups (Stata~16+).
\end{itemize}

\textbf{High-performance options} (Mata-based parser)
\begin{itemize}
    \item \texttt{bulk} activates the Mata-based bulk parser, which loads the entire file into memory for vectorized processing. This provides substantial performance gains on large files.
    \item \texttt{collapse} produces wide-format output with one row per top-level key, where each field becomes a column. Used with \texttt{bulk}.
    \item \texttt{colfields(string)} filters collapsed output to specific field names (semicolon-separated). For example, \texttt{colfields(code;name;source\_id)} retains only those three columns.
    \item \texttt{maxlevel(\#)} limits collapsed columns by nesting depth. Level~1 includes fields with no underscores; level~2 adds one underscore, and so on.
    \item \texttt{indicators} preset for wbopendata/unicefdata indicator metadata. Implies \texttt{bulk collapse} with a standard \texttt{colfields()} selection covering common metadata fields (code, name, source\_id, description, unit, topic\_ids, topic\_names, note, limited\_data).
    \item \texttt{strl} stores values as \texttt{strL} instead of \texttt{str2000}, allowing arbitrarily long values (useful for embedded documentation or description blocks).
\end{itemize}

\textbf{Example}
\begin{stlog}
. yaml read using "config.yaml", replace
(15 keys read from config.yaml)

. yaml read using "indicators.yaml", frame(ind)
(230 keys read from indicators.yaml into frame yaml_ind)

. yaml read using "indicators.yaml", fastread replace ///
      fields(name description source_id)
(733 entries read in fast-read mode)

. yaml read using "indicators.yaml", bulk collapse strl replace
(738 indicators, wide format)

. yaml read using "indicators.yaml", bulk collapse replace ///
      colfields(code;name;source_id;description)
(738 indicators, 4 fields)

. yaml read using "unicef_indicators.yaml", indicators replace
(738 indicators, wide format)
\end{stlog}

\textbf{Stored results}
\begin{itemize}
    \item \texttt{r(n\_keys)} — number of keys parsed
    \item \texttt{r(max\_level)} — maximum nesting depth
    \item \texttt{r(filename)} — name of file read
    \item \texttt{r(frame)} — name of frame created (if \texttt{frame()} is used)
    \item \texttt{r(cache\_hit)} — 1 if a cached result was returned (fast-read with \texttt{cache()})
\end{itemize}

Missing or unreadable files produce error messages and no data are imported.

\subsection{yaml write}

The \texttt{yaml write} subcommand exports a Stata dataset or frame to a YAML-formatted file. It recreates hierarchical structure based on the \texttt{key}, \texttt{parent}, and \texttt{level} variables.

\textbf{Syntax}
\begin{verbatim}
. yaml write using filename [, frame(name) scalars(namelist) 
      replace verbose indent(#) header(string)]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item \texttt{frame(name)} writes YAML constructed from frame \texttt{yaml\_name} (Stata~16+).
    \item \texttt{scalars(namelist)} includes named scalars as top-level YAML pairs.
    \item \texttt{replace} overwrites an existing file of the same name.
    \item \texttt{indent(\#)} sets the number of spaces per indent level (default: 2).
    \item \texttt{header(string)} adds a comment header to the top of the file.
\end{itemize}

\textbf{Example}
\begin{stlog}
. yaml write using "output.yaml", replace
(15 keys written to output.yaml)
\end{stlog}

If the dataset does not contain valid YAML variables (\texttt{key}, \texttt{value}, \texttt{parent}, \texttt{type}), \texttt{yaml write} returns an error.

\subsection{yaml describe}

The \texttt{yaml describe} subcommand (abbreviation: \texttt{yaml desc}) displays a compact tree-like representation of YAML data stored in the current dataset or a frame.

\textbf{Syntax}
\begin{verbatim}
. yaml describe [, frame(name) level(#)]
\end{verbatim}

\textbf{Example}
\begin{stlog}
. yaml describe
YAML structure (15 keys, max depth 3):
  name: My Project
  version: 1.0
  indicators/
    CME_MRY0T4/
      label: Under-five mortality rate
      unit: deaths per 1000 live births
\end{stlog}

The optional \texttt{level()} restricts output to a maximum indentation depth.

\subsection{yaml list}

The \texttt{yaml list} subcommand lists keys and/or values, optionally restricted by a parent key. It is useful for loops, metadata queries, and programmatic YAML exploration.

\textbf{Syntax}
\begin{verbatim}
. yaml list [parent] [, frame(name) keys values 
      separator(string) children stata noheader]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item \texttt{keys} returns key names in \texttt{r(keys)}.
    \item \texttt{values} returns values in \texttt{r(values)}.
    \item \texttt{children} returns only immediate children of the specified parent.
    \item \texttt{separator(string)} specifies the delimiter (default: space).
    \item \texttt{stata} formats returned lists in Stata compound quotes for looping.
    \item \texttt{noheader} suppresses printed output.
\end{itemize}

If the parent is not found, the returned lists are empty and \texttt{r(found)} = 0.

\textbf{Example}
\begin{stlog}
. yaml list indicators, keys children
Keys under indicators: CME_MRY0T4 CME_MRY0

. return list
r(keys) : "CME_MRY0T4 CME_MRY0"
\end{stlog}

\subsection{yaml get}

The \texttt{yaml get} subcommand retrieves all attributes (children) associated with a specific key and returns them in separate \texttt{r()} macros.

\textbf{Syntax}
\begin{verbatim}
. yaml get parent:keyname | keyname [, frame(name) attributes(namelist) quiet]
\end{verbatim}

The colon notation (\texttt{parent:key}) restricts the search to a specific branch.

If a key is not found, \texttt{r(found)} = 0 and no attributes are returned.

\textbf{Example}
\begin{stlog}
. yaml get indicators:CME_MRY0T4
  label: Under-five mortality rate
  unit: Deaths per 1000 live births
  dataflow: CME

. return list
r(key)      : "CME_MRY0T4"
r(parent)   : "indicators"
r(found)    : "1"
r(n_attrs)  : "3"
\end{stlog}

\subsection{yaml validate}

The \texttt{yaml validate} subcommand (abbreviation: \texttt{yaml check}) checks that specified keys exist and that selected keys match required value types.

\textbf{Syntax}
\begin{verbatim}
. yaml validate [, frame(name) required(keylist) 
      types(key:type ...) quiet]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item \texttt{required(keylist)} verifies that all listed keys exist.
    \item \texttt{types(key:type)} verifies that each key has the expected type (numeric, string, boolean).
    \item \texttt{quiet} suppresses printed output; results are returned silently in \texttt{r()}.
\end{itemize}

If validation fails, \texttt{r(valid)} = 0 and \texttt{r(n\_errors)} reports the number of issues.

\textbf{Example}
\begin{stlog}
. yaml validate, required(name version database) ///
>     types(database_port:numeric debug:boolean)
Validation passed (3 required keys, 2 type checks)
\end{stlog}

\subsection{yaml dir}

The \texttt{yaml dir} subcommand provides an inventory of all YAML data currently in memory, including data stored in the active dataset and in frames created via \texttt{yaml read}.

\textbf{Syntax}
\begin{verbatim}
. yaml dir [, detail]
\end{verbatim}

YAML data are detected by (1) the presence of standard YAML variables and (2) the dataset characteristic \texttt{\_dta[yaml\_source]} set by \texttt{yaml read}.

\textbf{Example}
\begin{stlog}
. yaml read using "config.yaml", replace
. yaml dir, detail
\end{stlog}

\subsection{yaml frames}

The \texttt{yaml frames} subcommand (abbreviation: \texttt{yaml frame}) lists only YAML-related frames, without reporting on the current dataset. This mirrors \texttt{frames dir} but is restricted to frames created by \texttt{yaml read}.

\textbf{Syntax}
\begin{verbatim}
. yaml frames [, detail]
\end{verbatim}

\textbf{Example}
\begin{stlog}
. yaml frames, detail
------------------------------------------------------------
YAML Frames in Memory
------------------------------------------------------------
  1. dev  (12 entries)
  2. prod (12 entries)
------------------------------------------------------------
\end{stlog}

\subsection{yaml clear}

The \texttt{yaml clear} subcommand removes YAML data from memory—either from the current dataset, from a specific frame, or from all YAML-related frames.

\textbf{Syntax}
\begin{verbatim}
. yaml clear [framename] [, all]
\end{verbatim}

\textbf{Options}
\begin{itemize}
    \item no argument — clear YAML variables from the current dataset.
    \item \texttt{framename} — drop the frame \texttt{yaml\_framename}.
    \item \texttt{all} — drop all frames created by \texttt{yaml read}.
\end{itemize}

\textbf{Example}
\begin{stlog}
. yaml clear config
(yaml_config dropped)

. yaml clear, all
(3 yaml frames dropped)
\end{stlog}


\section{Applications}
\label{sec:applications}

This section illustrates how the \texttt{yaml} command enables Stata to operate within configuration-driven, multi-language analytical workflows. The examples reflect common patterns in UNICEF systems, including SDMX-based indicator retrieval, survey microdata harmonization, pipeline validation, environment management through frames, and cross-platform standardization of Git-based repositories. Across these use cases, the separation of configuration from execution improves reproducibility, auditability, and interoperability in line with the UN Principles of Official Statistics.

\subsection{Indicator metadata management with the UNICEF SDMX API}

Many UNICEF analytics pipelines rely on structured metadata for indicator names, units, SDG mappings, and API-related details. YAML provides a single, readable format for storing such metadata in a form that can be shared across Stata, R, and Python. For example:

\begin{stlog}
indicators:
  CME_MRY0T4:
    name: Under-five mortality rate
    unit: Deaths per 1,000 live births
    dataflow: CME
    sdg_target: "3.2.1"
  NT_ANT_HAZ_NE2_MOD:
    name: Stunting prevalence (moderate and severe)
    unit: Percent
    dataflow: NUTRITION
    sdg_target: "2.2.1"
  IM_DTP3:
    name: DTP3 immunization coverage
    unit: Percent
    dataflow: IMMUNISATION
    sdg_target: "3.b.1"
\end{stlog}

The metadata can be imported into Stata and stored in a frame for later reference:

\begin{stlog}
. yaml read using "unicef_indicators.yaml", frame(meta)
(9 keys read into frame yaml_meta)

. yaml list indicators, keys children frame(meta)
Keys under indicators: CME_MRY0T4 NT_ANT_HAZ_NE2_MOD IM_DTP3
\end{stlog}

Because the SDMX REST API requires a dataflow identifier, YAML metadata can be used to construct URL queries programmatically:

\begin{stlog}
. local api_base "https://sdmx.data.unicef.org/ws/public/sdmxapi/rest/data"

. yaml get indicators:CME_MRY0T4, frame(meta) quiet
. local flow "`r(dataflow)'"
. local api_url "`api_base'/UNICEF,`flow',1.0/.CME_MRY0T4.?format=csv"

. import delimited "`api_url'", clear
(18 vars, 5,847 obs)
\end{stlog}

Labels can be drawn from the same YAML metadata:

\begin{stlog}
. yaml get indicators:CME_MRY0T4, frame(meta) quiet
. label variable obs_value "`r(name)' (`r(unit)')"
\end{stlog}

A full indicator batch can be looped over easily:

\begin{stlog}
. yaml list indicators, keys children frame(meta)
. local indicators "`r(keys)'"

. foreach ind of local indicators {
      yaml get indicators:`ind', frame(meta) quiet
      local flow "`r(dataflow)'"
      local url "`api_base'/UNICEF,`flow',1.0/.`ind'.?format=csv"

      import delimited "`url'", clear
      label variable obs_value "`r(name)' (`r(unit)')"
      label data "`r(name)' - SDG `r(sdg_target)'"
      save "`ind'.dta", replace
  }
\end{stlog}

This pattern ensures that indicator definitions remain consistent across the entire workflow and across analytical languages.

\textbf{One-step alternative.} For production use, the \texttt{indicators} preset provides a single-command shortcut that parses the entire catalog into a wide-format dataset:

\begin{stlog}
. yaml read using "unicef_indicators.yaml", indicators replace
(738 indicators, wide format)

. list key code name in 1/3
\end{stlog}

The preset automatically enables \texttt{bulk} and \texttt{collapse} with standard field selection, replacing the manual loop above when the full catalog is needed as a dataset rather than queried key-by-key.

\subsection{Efficient metadata ingestion: Large catalogs with frames}

While the preceding example illustrates simple indicator lookup, many data-production systems manage large metadata catalogs (often 700+ entries) that must be queried repeatedly. A naive approach—looping through entries and calling \texttt{yaml get} for each item—becomes slow at scale. This subsection illustrates an optimized pattern that leverages Stata's frame capability and direct dataset operations to achieve sub-second query performance on catalogs with thousands of entries.

\textbf{The problem.} Consider a system where data producers need to discover and filter all indicators in a specific dataflow category from a large, hierarchically-organized YAML catalog. A straightforward but inefficient approach would iterate over all indicators and invoke \texttt{yaml get} for each one:

\begin{stlog}
* Naive approach (inefficient for large catalogs):
. foreach indicator of local all_indicators {
      yaml get indicators:`indicator', frame(meta) quiet
      local category "`r(dataflow)'"
      if ("`category'" == "NUTRITION") {
          * collect results
      }
  }
  * With 733 indicators: 733 function calls × overhead = 15+ seconds
\end{stlog}

\textbf{The solution.} A more efficient approach leverages frames and direct dataset operations. By loading the entire YAML file into a frame once, and then querying it with vectorized Stata commands (rather than looping), the same operation completes in milliseconds:

\begin{stlog}
* Optimized approach (frame + direct query):
. yaml read using "indicators_catalog.yaml", frame(meta) verbose

. frame yaml_meta {
      * Generate match variable for all rows at once (vectorized)
      gen is_nutrition = (value == "NUTRITION") & ///
                         regexm(key, "^indicators_[A-Za-z0-9_]+_dataflow\$")

      * Extract indicator codes from matching keys (single operation)
      gen indicator_code = regexs(1) if ///
          regexm(key, "^indicators_([A-Za-z0-9_]+)_dataflow\$") & is_nutrition

      * Get all matched codes efficiently
      levelsof indicator_code if is_nutrition == 1, local(nutrition_codes) clean

      * For each matched indicator, fetch related metadata
      foreach ind of local nutrition_codes {
          levelsof value if key == "indicators_`ind'_name", local(ind_name) clean
          di "`ind': `ind_name'"
      }
  }

. frame drop yaml_meta  * Instant cleanup
\end{stlog}

\textbf{Performance comparison.} This pattern achieves a 50-fold speedup:

\begin{center}
\begin{tabular}{lcc}
\hline
\textbf{Operation} & \textbf{Time} & \textbf{Relative} \\
\hline
Load YAML once & 0.5 sec & --- \\
Naive: 733 iterative \texttt{yaml\_get} calls & 15+ sec & 50$\times$ \\
\textbf{Optimized}: Direct dataset query & 0.3 sec & \textbf{1$\times$} \\
\hline
\end{tabular}
\end{center}

\textbf{Why it is faster.} The performance gain stems from three factors: (1) \emph{vectorized operations}—Stata processes the entire column of keys at once, rather than in a loop; (2) \emph{single regex compilation}—the pattern is compiled once for all rows, not 733 times; and (3) \emph{no function-call overhead}—built-in Stata commands like \texttt{gen}, \texttt{regexm}, and \texttt{levelsof} are optimized for bulk operations. In contrast, repeated \texttt{yaml get} calls involve substantial macro expansion and function dispatch.

\textbf{Implementation details.} The key insight is that YAML hierarchies are flattened to underscore-separated keys, enabling regex-based pattern matching directly on the dataset. For example, a hierarchical indicator entry:

\begin{stlog}
indicators:
  CME_MRY0T4:
    name: Under-five mortality rate
    dataflow: CME
\end{stlog}

becomes three rows in the frame:

\begin{stlog}
key                        value                   
────────────────────────────────────────────────
indicators_CME_MRY0T4_name Under-five mortality...  
indicators_CME_MRY0T4_dataflow CME
\end{stlog}

The regex pattern \texttt{\^{}indicators\_[A-Za-z0-9\_]+\_dataflow\$} then matches rows where the final key component is the dataflow attribute, making it simple to filter and extract indicator codes.

\textbf{Frame management.} The use of frames provides additional benefits: (i) \emph{isolation}—the working dataset is untouched, reducing risk of data corruption; (ii) \emph{cleanup}—a single \texttt{frame drop} removes all metadata instantly; (iii) \emph{multi-source}—multiple YAML files can coexist in separate frames; and (iv) \emph{compatibility}—for Stata versions prior to 16, \texttt{preserve}/\texttt{restore} provides equivalent functionality.

This pattern is now widely used in production systems (e.g., \texttt{unicefdata} v1.4.0) to filter 700+ indicator catalogs at scale while maintaining code clarity and reproducibility. It demonstrates how thoughtful integration of YAML with Stata's frame and command architecture can achieve substantial performance and maintainability gains in modern data-production workflows.

\textbf{Scaling to production catalogs.} Real-world indicator registries operate at substantially larger scales. The \texttt{wbopendata} package \citep{azevedo2011wbopendata} maintains a catalog of approximately 29,000 World Bank indicators stored in YAML files exceeding 360,000 lines, while \texttt{unicefdata} \citep{azevedo2025unicefdata} provides access to 700+ UNICEF indicators. These catalogs benefit from the high-performance Mata parser:

\begin{stlog}
. yaml read using "wdi_catalog.yaml", indicators replace
(29,189 indicators, wide format)

. yaml read using "unicef_catalog.yaml", indicators replace
(738 indicators, wide format)
\end{stlog}

The \texttt{indicators} preset automatically enables \texttt{bulk} and \texttt{collapse} with a standard \texttt{colfields()} selection covering the most common metadata fields. For custom field selection, the \texttt{colfields()} and \texttt{maxlevel()} options provide fine-grained control over which columns appear in the collapsed output. The \texttt{strl} option stores values as strLs for catalogs with long description fields, avoiding Stata's 2,045-character string limit. This combination makes it practical to load, filter, and query indicator metadata at scales previously impractical in Stata. To ensure that future \texttt{yaml} updates do not break these production workflows, compatibility tests covering frame-based query patterns are maintained as part of the QA framework (see Appendix~\ref{sec:qa}).

\subsection{YAML-driven microdata harmonization}
\label{sec:microdata}

Harmonizing survey microdata often requires mapping a variety of source variables to harmonized names, applying consistent labels, and recoding categorical values. Encoding this logic in YAML keeps harmonization decisions separate from the Stata code that executes them.

\textbf{Source specifications.} Each country survey uses instrument-specific variable names and coding schemes. YAML files capture the original structure for each source, following IHSN naming conventions (\texttt{country\_year\_survey}):

\begin{stlog}
# ETH_2019_MICS6_source.yaml - Ethiopia MICS6 variable mapping
country: ETH
year: 2019
survey: MICS6
ihsn_id: ETH_2019_MICS6
input_file: data/raw/ETH_2019_MICS6.dta

variables:
  hhid:
    from: HH1
  male:
    from: HL4
    values:
      1: 1
      2: 0
\end{stlog}

\begin{stlog}
# GHA_2018_MICS6_source.yaml - Ghana MICS6 variable mapping
country: GHA
year: 2018
survey: MICS6
ihsn_id: GHA_2018_MICS6
input_file: data/raw/GHA_2018_MICS6.dta

variables:
  hhid:
    from: HH1
  male:
    from: HL4
    values:
      1: 1
      2: 0
\end{stlog}

\textbf{Target schema.} The harmonized output conforms to a standardized, country-agnostic schema:

\begin{stlog}
# harmonized_schema.yaml - Target schema (applies to all countries)
schema_version: "1.0"
survey_program: MICS6

variables:
  hhid:
    type: numeric
    label: Household ID
  male:
    type: categorical
    label: Sex (1=Male, 0=Female)
    values:
      0: Female
      1: Male
\end{stlog}

A harmonization script uses the mapping configuration as follows:

\begin{stlog}
. yaml read using "ETH_2019_MICS6_source.yaml", frame(src)
. yaml get input_file, frame(src) quiet
. use "`r(value)'", clear

. yaml list variables, keys children frame(src)
. local varlist "`r(keys)'"

. foreach var of local varlist {
      yaml get variables:`var':from, frame(src) quiet
      rename `r(value)' `var'

      yaml list variables:`var':values, keys children frame(src)
      if "`r(keys)'" != "" {
          local recode_str ""
          foreach v of local r(keys) {
              yaml get variables:`var':values:`v', frame(src) quiet
              local recode_str "`recode_str' (`v' = `r(value)')"
          }
          recode `var' `recode_str'
      }
  }
\end{stlog}

This separation of **harmonization logic** (YAML) from **execution** (Stata) improves traceability, documentation, and cross-language interoperability.

\textbf{Automated code generation.} For large-scale harmonization programs spanning multiple countries and survey rounds, manually maintaining do-files becomes unwieldy. An ado-file can read the source mappings and target schema, then generate self-contained harmonization scripts for each country-year-survey combination. This pattern follows the International Household Survey Network (IHSN) microdata documentation standards \citep{ihsn2024standards}, which emphasize traceable, reproducible data processing with clear separation of raw and harmonized datasets.

The generator reads a registry of source configurations and a target schema:

\begin{stlog}
. yaml read using "harmonized_schema.yaml", frame(target)
. yaml read using "registry.yaml", frame(registry)

. yaml list surveys, keys children frame(registry)
. local surveys "`r(keys)'"

. foreach s of local surveys {
      yaml get surveys:`s':source_yaml, frame(registry) quiet
      local src_yaml "`r(value)'"
      yaml get surveys:`s':ihsn_id, frame(registry) quiet
      local ihsn_id "`r(value)'"

      * Generate do-file for this survey (IHSN naming: country_year_survey)
      generate_harmonization_do, ///
          source("`src_yaml'") ///
          schema(target) ///
          output("dofiles/`ihsn_id'_harm.do")
  }
\end{stlog}

The \texttt{generate\_harmonization\_do} program (distributed separately) iterates over variables in the source mapping, cross-references against the target schema, and emits a complete do-file with:

\begin{itemize}
    \item Header comments documenting survey metadata, generation timestamp, and schema version
    \item \texttt{use} command for the raw input file
    \item \texttt{rename} commands from original to harmonized variable names
    \item \texttt{recode} commands for categorical value mappings
    \item \texttt{label variable} and \texttt{label define/values} commands from the target schema
    \item \texttt{save} command using IHSN naming conventions (\texttt{harmonized/\{country\}\_\{year\}\_\{survey\}\_harm.dta})
\end{itemize}

An example generated do-file:

\begin{stlog}
* ETH_2019_MICS6_harm.do - Auto-generated harmonization script
* IHSN ID: ETH_2019_MICS6
* Source: ETH_2019_MICS6_source.yaml | Schema: harmonized_schema v1.0
* Generated: 2026-02-20 17:15:00

use "data/raw/ETH_2019_MICS6.dta", clear

* Variable: hhid (from HH1)
rename HH1 hhid
label variable hhid "Household ID"

* Variable: male (from HL4)
rename HL4 male
recode male (1 = 1) (2 = 0)
label define male_lbl 0 "Female" 1 "Male"
label values male male_lbl
label variable male "Sex (1=Male, 0=Female)"

compress
save "harmonized/ETH_2019_MICS6_harm.dta", replace
\end{stlog}

This approach ensures that harmonization logic remains auditable in YAML while execution artifacts (do-files) are regenerated deterministically from source configurations. Changes to mappings propagate automatically on regeneration, reducing manual errors and supporting quality assurance workflows required by IHSN and similar standards.

\subsection{Configuration validation}

In automated pipelines, configuration files often require strict validation before processing begins. The \texttt{yaml validate} subcommand provides a convenient mechanism:

\begin{stlog}
. yaml read using "pipeline_config.yaml", replace

. yaml validate, required(name version database api_endpoint) ///
      types(database_port:numeric api_timeout:numeric debug:boolean)

. if (r(valid) == 0) {
      di as error "Configuration validation failed!"
      exit 198
  }
\end{stlog}

Validation ensures that configuration errors are caught early, supporting transparent governance and predictable execution.

\subsection{Working with multiple YAML files}

Many production pipelines maintain separate development and production configurations. With Stata frames, multiple configurations can be loaded simultaneously:

\begin{stlog}
. yaml read using "dev_config.yaml", frame(dev)
. yaml read using "prod_config.yaml", frame(prod)
\end{stlog}

A consolidated inventory can be obtained using:

\begin{stlog}
. yaml frames, detail
------------------------------------------------------------
YAML Frames in Memory
------------------------------------------------------------
  1. dev  (12 entries)
  2. prod (12 entries)
------------------------------------------------------------
\end{stlog}

Environment-specific settings can then be inspected or compared programmatically:

\begin{stlog}
. yaml get database:host, frame(dev)
. local dev_host "`r(host)'"

. yaml get database:host, frame(prod)
. local prod_host "`r(host)'"

. di "Dev: `dev_host', Prod: `prod_host'"
Dev: localhost, Prod: db.production.example.com
\end{stlog}

This pattern supports clean separation of configuration by environment.

\subsection{Round-trip: reading and writing YAML from Stata}

Beyond consuming YAML, Stata can also modify and regenerate configuration files, enabling it to participate actively in configuration management:

\begin{stlog}
. yaml read using "config.yaml", replace

. replace value = "3.0" if key == "version"
. replace value = "`c(current_date)'" if key == "last_modified"

. yaml write using "config_updated.yaml", replace
\end{stlog}

Round-trip processing is useful for timestamping, version increments, and recording workflow status indicators in automated pipelines.

\subsection{Standardizing Git-based project profiles across tools}

UNICEF analytical projects frequently span Stata, R, and Python and are managed in Git-based repositories. Each analyst typically maintains a local clone and personal file paths. Historically, Stata profile do-files contained long username-specific blocks to resolve folder locations. These were difficult to scale and error-prone.

A YAML configuration kept outside the repository provides a more robust and interoperable solution:

\begin{stlog}
\# ~/.config/user_config.yml

analyst1:
  githubFolder: "D:/Users/analyst1/GitHub"
  teamsRoot:   "C:/Users/analyst1/TeamsRoot"

analyst2:
  githubFolder: "C:/Users/analyst2/Projects/GitHub"
  teamsRoot:   "C:/Users/analyst2/Shared/TeamsRoot"
\end{stlog}

Stata can then resolve the correct paths dynamically:

\begin{stlog}
. local user = c(username)
. yaml read using "~/.config/user_config.yml", frame(usercfg)
. yaml get `user', frame(usercfg) quiet
. cd "`r(githubFolder)'/ProjectName"
\end{stlog}

The same YAML configuration can be consumed by R:

\begin{stlog}
cfg  <- yaml::read_yaml("~/.config/user_config.yml")
user <- Sys.getenv("USERNAME")
root <- cfg[[user]]\$githubFolder
\end{stlog}

and Python:

\begin{stlog}
with open("~/.config/user_config.yml") as f:
    cfg = yaml.safe_load(f)
root = cfg[os.environ["USERNAME"]]["githubFolder"]
\end{stlog}

This practice establishes a single source of truth for environment-specific paths, strengthening reproducibility, reducing setup friction for new team members, and supporting consistent behavior across Stata, R, and Python in line with the UN Principles of Official Statistics.

\subsection*{Recommended practices}

In practice, we have found a few patterns particularly useful when integrating \texttt{yaml} into Stata workflows: (i) store shared analytical configuration (indicator lists, harmonization maps, pipeline toggles) in YAML files under version control; (ii) store user- and machine-specific settings (local paths, credentials) in YAML files outside the repository, as in the Git-based profile example; and (iii) keep secrets and tokens in separate, access-controlled configuration files rather than in project-level YAML. Together, these patterns help implement a “single source of truth” for configuration while aligning with good practices for reproducibility and data governance.

\section{Discussion}
\label{sec:discussion}
The \texttt{yaml} command positions Stata as a first-class participant in modern multi-language analytical ecosystems. Although YAML has long been central to workflows in R, Python, and cloud-computing environments, Stata has lacked native support for reading or writing YAML-based configuration files. The implementation presented here follows established Stata programming conventions \citep{cox05d} and provides a lightweight bridge between Stata and the broader reproducible-research infrastructure increasingly used across data science and official statistics.

Several features of the design are particularly valuable for empirical research:

\begin{itemize}
    \item \emph{Portability}: YAML is widely supported, human-readable, and naturally version-controlled.
    \item \emph{Reproducibility}: externalizing configuration parameters reduces hidden assumptions and promotes stable, auditable workflows.
    \item \emph{Interoperability}: Stata can now consume the same configuration artifacts used by R, Python, Quarto, GitHub Actions, and cloud-orchestration tools.
    \item \emph{Transparency}: the chosen JSON-compatible subset of YAML is simple enough to inspect and edit by hand, aligning with best practices for documentation and governance.
    \item \emph{No dependencies}: the implementation uses only built-in Stata functionality, avoiding reliance on Python or external libraries.
\end{itemize}

The design also resonates with \citet{knuth84}'s principles of literate programming, which emphasize making analytical logic understandable to humans rather than merely executable by machines. YAML-based configuration naturally supports this philosophy: by externalizing parameters, metadata, and workflow settings into readable files, analysts create systems in which the ``what'' and ``why'' of an analysis are documented alongside the ``how.'' The microdata harmonization example in Section~\ref{sec:applications} illustrates this directly, showing how YAML can simultaneously serve as configuration, documentation, and audit trail. This separation of concerns reflects widely endorsed principles for transparent statistical production, including the United Nations Fundamental Principles of Official Statistics.

The current implementation has limitations. The canonical parser deliberately targets only the JSON-schema subset of the YAML~1.2 specification (Chapter~10.2). Advanced features such as anchors, aliases, explicit type tags, and flow-style mappings are not supported. Block scalars (\texttt{|} and \texttt{>}) are supported in both fast-read mode (via the \texttt{blockscalars} option) and in the canonical parser. Parsing relies on indentation rules, so malformed YAML results in structured, human-readable error messages rather than partial recovery. The fast-read parser is optimized for shallow two-level mappings and list blocks; deeply nested or irregular structures should use the canonical parser. These constraints reflect a deliberate focus on simplicity, predictability, and reproducibility for configuration files, even though they may limit applicability to more complex or free-form YAML documents.

In practice, YAML configuration files used in UNICEF and similar analytical pipelines range from modest (tens of keys) to large (700+ indicator entries). On small files, parsing and frame creation typically complete in well under a second on standard hardware. For large metadata catalogs, the fast-read mode with field-selective extraction and frame-based caching provides near-linear performance, enabling sub-second ingestion of files with hundreds of entries. The Mata-based bulk parser (\texttt{bulk} option) provides additional performance gains by loading the entire file into memory and using vectorized Mata operations; on catalogs with 800+ indicators, it achieves parsing times under two seconds. The \texttt{collapse} option then reshapes this output into a wide format suitable for direct querying with standard Stata commands. The canonical parser also offers targeted parsing (\texttt{targets()} with \texttt{earlyexit}) and streaming tokenization (\texttt{stream}) for improved performance on large files without switching to the fast-read schema. The implementation does not depend on external libraries, so performance scales approximately with the number of keys and lines in the YAML file.

An alternative approach is to rely on Stata’s integration with external languages (for example, calling PyYAML via \texttt{python}) or to constrain configuration to JSON only. These strategies can be effective in some settings but introduce external dependencies, complicate deployment in secure or server-based environments, and limit the ability to inspect or edit configuration directly from within Stata. The \texttt{yaml} command complements such approaches by providing a native interface to a widely used configuration language while maintaining maximal portability.

YAML is also increasingly central to artificial-intelligence and 
large-language-model workflows, where it is used to specify tool 
interfaces, agent configurations, and orchestration logic for multi-step 
pipelines. As these systems become more integrated into statistical 
production and analytical research, native YAML support helps ensure that 
Stata can interoperate with—and contribute to—such emerging infrastructures.


Future extensions could include:

\begin{itemize}
    \item helper tools for emitting YAML variants tailored to specific infrastructures (such as GitHub Actions or Quarto metadata blocks);
    \item schema validation against user-defined constraints for configuration auditing;
    \item YAML diff and compare operations for tracking configuration changes across environments; and
    \item interactive tools for inspecting or editing YAML directly within Stata.
\end{itemize}

These enhancements would further strengthen Stata’s interoperability with configuration-driven pipelines and expand the scope of workflows that can be expressed natively in YAML.


\section{Conclusions}
\label{sec:conclusions}

As analytical pipelines increasingly span multiple programming environments, the ability to consume and generate human-readable configuration files has become essential for reproducible research. YAML has emerged as a standard configuration language across data science, underpinning workflows in R, Python, Quarto, DevOps systems, and cloud infrastructure. The \texttt{yaml} command introduced in this article brings this capability to Stata in a lightweight and dependency-free manner.

The unified subcommand architecture\textemdash comprising \texttt{read}, \texttt{write}, \texttt{describe}, \texttt{list}, \texttt{get}, \texttt{validate}, \texttt{dir}, \texttt{frames}, and \texttt{clear}\textemdash provides a comprehensive yet coherent interface for working with YAML files. Version~1.9.0 extends this foundation with a high-performance Mata-based bulk parser, wide-format output via the \texttt{collapse} option, collapse filter options (\texttt{colfields}, \texttt{maxlevel}), an \texttt{indicators} preset for one-step metadata ingestion, and \texttt{strL} storage for arbitrarily long values. This design allows analysts to externalize configuration parameters, metadata structures, and validation rules, thereby improving reproducibility, transparency, and auditability within statistical workflows. Stata scripts can now participate seamlessly in multi-language pipelines, consume configuration artifacts shared across tools, and even update pipeline metadata programmatically.

By enabling Stata to interoperate directly with YAML-based workflows, the \texttt{yaml} command strengthens its role in interdisciplinary analytical ecosystems and supports good statistical governance practices, including those emphasized by the United Nations Fundamental Principles of Official Statistics. The approach facilitates cleaner metadata management, more flexible and maintainable pipeline design, and more consistent results across platforms—an increasingly important requirement in collaborative empirical research.

This capability is particularly relevant for national statistical offices and international agencies that rely on mixed-language pipelines to produce official statistics. Clear configuration management and cross-tool coherence enhance the transparency, reproducibility, and professional standards required for the production of credible and trustworthy official statistics.


\section*{Acknowledgments}

The author thanks the Stata Journal editors and anonymous reviewers for their constructive feedback. The views expressed in this article are those of the author and do not necessarily reflect the official position of UNICEF.

\bibliographystyle{sj}
\bibliography{references_yaml}


\begin{aboutauthor}
Jo\~ao Pedro Azevedo is Deputy Director and Chief Statistician in UNICEF's Division of Data, Analytics, Planning and Monitoring. He works on official statistics and the development of reproducible, cross-platform, and scalable analytical pipelines that support the production and use of child-related data in global monitoring systems, with the aim of generating policy-relevant insights and informing action on the ground.
\end{aboutauthor}

\appendix
\renewcommand{\thesection}{A}
\renewcommand{\thesubsection}{A.\arabic{subsection}}

\section{Quality assurance framework}
\label{sec:qa}

The \texttt{yaml} module includes a comprehensive quality assurance (QA) framework designed to ensure correctness across all subcommands and both parser modes. The framework follows a test-driven methodology with automated regression testing.

\subsection{Architecture}

The QA system comprises five test families covering distinct aspects of functionality:

\begin{center}
\begin{tabular}{llcl}
\hline
\textbf{Family} & \textbf{Scope} & \textbf{Tests} & \textbf{Description} \\
\hline
ENV   & Environment    & 3  & Module installation and discoverability \\
EX    & Examples       & 3  & Smoke tests for core workflows \\
REG   & Regression     & 8  & Bug-fix validation \\
FEAT  & Features       & 9  & New functionality validation \\
INT   & Integration    & 3  & Cross-package compatibility (wbopendata, unicefdata) \\
\hline
      & \textbf{Total} & 26 & \\
\hline
\end{tabular}
\end{center}

\subsection{Test execution}

Tests are executed via a unified runner:

\begin{stlog}
. cd C:/GitHub/myados/yaml-dev/qa
. do run_tests.do         // Full suite (26 tests)
. do run_tests.do FEAT-08 // Single test
. do run_tests.do list    // List available tests
\end{stlog}

The runner produces structured logs in \texttt{qa/logs/} and appends results to a tracked \texttt{test\_history.txt} file for audit purposes.

\subsection{Coverage highlights}

\textbf{Regression tests} (REG-01 through REG-08) target specific bug fixes, ensuring that resolved issues do not reoccur. Each regression test references a documented bug identifier and includes both the failing case and the expected correct behavior.

\textbf{Feature tests} (FEAT-01 through FEAT-09) validate new capabilities introduced in v1.6.0 and later. FEAT-08 alone contains 15 sub-tests covering frame-based query patterns used in production by \texttt{wbopendata} and \texttt{unicefdata}. These tests serve as a compatibility contract: they ensure that future updates to the \texttt{yaml} module will not break the required functionality of these downstream packages, which depend on specific parsing behaviors and frame operations for managing large indicator catalogs:

\begin{itemize}
    \item Bulk parse with \texttt{bulk} + \texttt{collapse} options
    \item Frame cache operations (put/get pattern)
    \item Keyword, topic, and regex-based indicator search
    \item Multi-field search (name AND description)
    \item List field parsing (semicolon-delimited values)
    \item Frame persistence across \texttt{clear}
\end{itemize}

\subsection{Results summary}

As of v1.9.0, all 26 tests pass on Stata 17+ and Stata 18. Frame-dependent tests (REG-02, FEAT-08) require Stata~16+; integration tests (INT-01 through INT-03) require the corresponding downstream packages. Tests are automatically skipped on earlier versions or when dependencies are unavailable, with appropriate warnings.

\begin{center}
\begin{tabular}{lcc}
\hline
\textbf{Test Family} & \textbf{Passed} & \textbf{Total} \\
\hline
ENV  & 3 & 3 \\
EX   & 3 & 3 \\
REG  & 8 & 8 \\
FEAT & 9 & 9 \\
INT  & 3 & 3 \\
\hline
\textbf{All} & \textbf{26} & \textbf{26} \\
\hline
\end{tabular}
\end{center}

The QA framework documentation and test scripts are distributed with the module in the \texttt{qa/} directory, enabling users to verify correct installation and behavior in their local environment.

\subsection{Test fixtures}

The test suite uses purpose-built YAML fixtures stored in \texttt{qa/fixtures/}. Each fixture isolates a specific parsing scenario or edge case:

\begin{center}
\begin{tabular}{ll}
\hline
\textbf{Fixture} & \textbf{Purpose} \\
\hline
\texttt{block\_scalars.yaml} & Literal (\texttt{|}) and folded (\texttt{>}) block syntax \\
\texttt{brackets\_in\_values.yaml} & REG-06: \texttt{[]} and \texttt{\{\}} preserved in values \\
\texttt{collapse\_test.yaml} & Wide-format \texttt{collapse} option output \\
\texttt{collapse\_colfields\_test.yaml} & FEAT-09: \texttt{colfields()} and \texttt{maxlevel()} filtering \\
\texttt{continuation\_lines.yaml} & Multi-line string handling \\
\texttt{embedded\_quotes.yaml} & FEAT-01: double quotes via Mata \\
\texttt{long\_values.yaml} & FEAT-04: strL truncation prevention \\
\texttt{validate\_types.yaml} & REG-05: type validation matching \\
\texttt{\_wbopendata\_indicators.yaml} & Large catalog performance (optional) \\
\hline
\end{tabular}
\end{center}

For portability, fixtures are distributed as \texttt{fixtures.zip} and automatically extracted on first test run. The optional \texttt{\_wbopendata\_indicators.yaml} fixture (29,000+ entries) enables large-scale performance testing but is skipped by default to avoid slow test runs. Users can enable it by placing the file in the fixtures directory before running FEAT-08.

\subsection{Relation to downstream package testing}

The \texttt{yaml} module serves as a foundational dependency for \texttt{wbopendata} and \texttt{unicefdata}, which maintain their own comprehensive testing frameworks. The \texttt{wbopendata} QA suite comprises 89 tests including deterministic offline tests using pre-recorded fixtures, while the \texttt{unicefdata} ecosystem employs a cross-platform testing framework with 443 tests spanning Stata, Python, and R implementations.

The FEAT-08 compatibility tests in this module are designed to mirror the frame-based query patterns used by these downstream packages. When the downstream test suites pass, users can be confident that the full stack---from YAML parsing to indicator discovery to API response processing---operates correctly. Conversely, the yaml-level tests serve as an early warning system: a failure in FEAT-08 would signal potential breaking changes before they propagate to production workflows.

